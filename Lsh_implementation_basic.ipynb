{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    Implementing a simplified version of Locality sensitive hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Sentences\n",
    "a = \"The quick brown fox jumped over the little lazy dog\"\n",
    "b = \"An elephant is jumping around in the streets\"\n",
    "c = \"You have to be quick, being lazy will take you nowhere\"\n",
    "d = \"The little lazy dog jumped over the quick brown fox\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be make k shingles with value of k being = 3. For larger datasets with large texts, we have to take the size of k more so that only those texts which are similar get closer vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "def shingle(text: str, k = 3):\n",
    "    shingle_list = []\n",
    "    for i in range(0,len(text) - k):\n",
    "        shingle_list.append(text[i:i+k])\n",
    "    return list(set(shingle_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uic', ' fo', 'lit', ' br', 'ped', 'le ', 'ox ', 'ove', 'own', ' li', ' la', 'azy', 'mpe', 'e l', 'he ', ' ju', 'zy ', 'fox', 'The', 'tle', ' do', 'y d', 'r t', 'ick', 'ck ', 'k b', 'n f', 'wn ', 'ump', ' qu', 'bro', 'x j', ' ov', 'ver', ' th', 'itt', 'ttl', 'ed ', 'laz', 'row', 'e q', 'jum', 'd o', 'the', 'qui', 'er ']\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of formation of shingles\n",
    "A = shingle(a,k)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def vocab(lest : list, k: int):\n",
    "    newlist = []\n",
    "    for texts in lest:\n",
    "        shin = shingle(texts, k)\n",
    "        for shingl in shin:\n",
    "            newlist.append(shingl)\n",
    "    thelist = list(set(newlist))\n",
    "    random.shuffle(thelist) \n",
    "    return thelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be ', ' yo', ' in', 'dog', 'o b', ' br', ' ar', 'e t', 'own', 'ele', 'An ', 'nt ', 'qui', 'le ', 'wn ', ' is', 'tak', 'k, ', 'ke ', 'e s', 'whe', ' ha', 'og ', 'is ', 'wil', 'g l', 'k b', 'fox', ', b', 'ump', 'ake', 'ped', 't i', 'ed ', ' wi', 'oun', ' fo', 'her', 'hav', 'y d', 'er ', 'ou ', 'the', 've ', 'eet', 'now', 'ox ', 'pin', 'r t', ' st', 'itt', 'tre', 'zy ', 'he ', 'e q', 'ng ', ' ov', 'n e', 'e l', 'ick', 'ove', 'bei', 'The', 'aro', 'ver', 'nd ', 'u n', ' do', 'll ', 'mpe', 'd o', ' ju', ' li', 'g a', 'bro', 'ck ', 'n t', ' ta', 'x j', 'ein', 'ill', 'ant', ' el', 'n f', 's j', 'laz', 'han', 'mpi', 'to ', 'jum', 'in ', ' be', 'pha', 'str', 'eph', 'uic', 'lep', ' no', 'ave', 'ck,', 'owh', 'lit', 'd i', ' to', 'y w', ' la', 'row', 'tle', 'rou', 'e y', ' th', 'g j', 'ree', 'ing', ' qu', 'und', 'l t', 'azy', 'u h', 'you', 'You', 'ttl']\n",
      "length of vocab is:  122\n"
     ]
    }
   ],
   "source": [
    "texts = [a,b,c,d]\n",
    "vocab = vocab(texts, k)\n",
    "print(vocab)\n",
    "print(\"length of vocab is: \", len(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            Creating One hot vectors for all the text sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "one_hot_vectors = []\n",
    "for text in texts:\n",
    "    a_1hot = [1 if x in text else 0 for x in vocab]\n",
    "    one_hot_vectors.append(a_1hot)\n",
    "\n",
    "print(len(one_hot_vectors))\n",
    "print(len(one_hot_vectors[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 4 122 dimensioned vectors which are one hot encoded for the k shingles. We will now use minhashing to get the different types of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINHASHING to get signature\n",
    "import numpy as np\n",
    "\n",
    "def findones(arr, lest):\n",
    "    for i in arr:\n",
    "        if(lest[i] == 1):\n",
    "            return int(i)\n",
    "\n",
    "def get_signature(lest: list, n = 20):\n",
    "    length = len(lest[0])\n",
    "    random_matrix = []\n",
    "    for i in range(n):\n",
    "        random_matrix.append(np.random.randint(0,length,length))\n",
    "    signatures = []\n",
    "    for les in lest:\n",
    "        sig = np.ones(n)\n",
    "        for i in range(n):\n",
    "            sig[i] = findones(random_matrix[i], les)\n",
    "        signatures.append(sig)\n",
    "    return signatures\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 89.,  33.,  12.,   5.,  46.,  70.,  78.,  12.,  70.,  33.,  83.,\n",
      "        14.,  31.,  72., 117.,  58., 117.,  85.,  46.,  95.]), array([ 89.,  35.,  55.,  11.,   6.,  93.,  71.,  32.,  44.,   9.,  65.,\n",
      "        42.,   6.,  63.,  19.,  49., 112.,  93.,  96., 115.]), array([ 91., 100.,  12.,  21.,  77.,  20., 103.,  12., 116.,   4.,  79.,\n",
      "       100.,   4.,  55., 117.,  12.,  41.,  85.,  88., 116.]), array([ 89.,  33.,  12.,   5.,  40.,  70.,  71.,  12.,  70.,  33.,  83.,\n",
      "        14.,  22.,  72., 117.,  58., 111., 111.,  40.,  95.])]\n"
     ]
    }
   ],
   "source": [
    "g = get_signature(one_hot_vectors)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard function to compare two vectors\n",
    "def jaccard(a, b):\n",
    "    ans = 0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] in b:\n",
    "            ans = ans +1\n",
    "    return ans/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13043478260869565 0.05\n",
      "0.1956521739130435 0.25\n",
      "0.9347826086956522 0.75\n"
     ]
    }
   ],
   "source": [
    "# Comparing a and b\n",
    "print(jaccard(shingle(a), shingle(b)), jaccard(g[0], g[1]))\n",
    "# Comparing a and c\n",
    "print(jaccard(shingle(a), shingle(c)), jaccard(g[0], g[2]))\n",
    "# Comparing a and d\n",
    "print(jaccard(shingle(a), shingle(d)), jaccard(g[0], g[3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have shown that texts which match in their semantics also match in their hashing, so we will use these signatures and Perform an LSH (Locality Sensitiv Hashing on them)\n",
    "\n",
    "We will make bands of the subvectors and will make candidate pairs if the hasing of the subvecotrs falls into the same bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vector(signature, b):\n",
    "    assert len(signature) % b == 0\n",
    "    r = int(len(signature) / b)\n",
    "    # code splitting signature in b parts\n",
    "    subvecs = []\n",
    "    for i in range(0, len(signature), r):\n",
    "        subvecs.append(signature[i : i+r])\n",
    "    return subvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = 10\n",
    "bands_a = split_vector(g[0], bands)\n",
    "bands_b = split_vector(g[1], bands)\n",
    "bands_c = split_vector(g[2], bands)\n",
    "bands_d = split_vector(g[3], bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([89., 33.]), array([12.,  5.]), array([46., 70.]), array([78., 12.]), array([70., 33.]), array([83., 14.]), array([31., 72.]), array([117.,  58.]), array([117.,  85.]), array([46., 95.])]\n",
      "[array([89., 35.]), array([55., 11.]), array([ 6., 93.]), array([71., 32.]), array([44.,  9.]), array([65., 42.]), array([ 6., 63.]), array([19., 49.]), array([112.,  93.]), array([ 96., 115.])]\n"
     ]
    }
   ],
   "source": [
    "print(bands_a)\n",
    "print(bands_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_bands(banda, bandb):\n",
    "#     for i in range(len(banda)):\n",
    "#         equal = 1\n",
    "#         for j in range(len(banda[i])):\n",
    "#             if banda[i] != bandb[i]:\n",
    "#                 equal = 0\n",
    "#                 break;\n",
    "\n",
    "#         if banda[i].all() == bandb[i].all():\n",
    "#             print(banda[i].all())\n",
    "#             print(bandb[i].all())\n",
    "#             print(\"Candidate pairs \", banda[i])\n",
    "#             return\n",
    "#     print(\"Not Candidate pairs\")\n",
    "\n",
    "def check_bands(banda, bandb):\n",
    "    for i in range(len(banda)):\n",
    "        if list(banda[i]) == list(bandb[i]):\n",
    "            print(\"Candidate pairs \", banda[i])\n",
    "            return\n",
    "    print(\"Not Candidate pairs\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Candidate pairs\n",
      "Not Candidate pairs\n",
      "Candidate pairs  [89. 33.]\n",
      "Not Candidate pairs\n"
     ]
    }
   ],
   "source": [
    "# Checking the candidate pairs of statements\n",
    "check_bands(bands_a, bands_b)\n",
    "check_bands(bands_a, bands_c)\n",
    "check_bands(bands_a, bands_d)\n",
    "check_bands(bands_b, bands_d)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hense, we can see sentence a = \"The quick brown fox jumped over the little lazy dog\" and sentence \n",
    "d = \"The little lazy dog jumped over the quick brown fox\" are a candidate pair"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7a0f0345bf008463265b97b79e6b6ac46fd48f5252c12e26d20b6a21351a366"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
